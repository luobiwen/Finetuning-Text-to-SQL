{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 加载基础模型（这个文件很大，需要另外下载）\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = PeftModel.from_pretrained(base_model, \"sqlflow-qwen-lora-final\")",
   "id": "f1778492edf59ba3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "测试（生成sql查询）",
   "id": "ca64c0796ad56db4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_sql(question, db_schema=None):\n",
    "    \"\"\"\n",
    "    生成SQL查询\n",
    "    \"\"\"\n",
    "    # 准备输入\n",
    "    if db_schema:\n",
    "        input_text = f\"问题: {question}\\n数据库结构: {db_schema}\\nSQL:\"\n",
    "    else:\n",
    "        input_text = question\n",
    "\n",
    "    # 编码输入\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "    # 生成SQL\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=256,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.1,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    # 解码输出\n",
    "    sql_query = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return sql_query\n",
    "\n",
    "# 使用示例\n",
    "question = \"查询销售部门工资最高的员工\"\n",
    "db_schema = \"employees(id, name, department, salary), departments(id, name)\"\n",
    "\n",
    "sql = generate_sql(question, db_schema)\n",
    "print(f\"问题: {question}\")\n",
    "print(f\"生成的SQL: {sql}\")"
   ],
   "id": "4f63c2fa12417e4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1.根据表名获取表结构，解析schema文档",
   "id": "50bf9dd45298bc39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def get_table_schema(md_content, table_name):\n",
    "    \"\"\"\n",
    "    根据表名从MD内容中提取完整的表结构\n",
    "\n",
    "    Args:\n",
    "        md_content: MD文档内容\n",
    "        table_name: 表名\n",
    "\n",
    "    Returns:\n",
    "        表结构的字符串表示\n",
    "    \"\"\"\n",
    "    # 构建表头的正则表达式模式\n",
    "    table_header_pattern = rf'# Table:\\s*{re.escape(table_name)}\\s*\\n\\s*\\['\n",
    "\n",
    "    # 查找表开始位置\n",
    "    table_start = re.search(table_header_pattern, md_content)\n",
    "    if not table_start:\n",
    "        return f\"表 '{table_name}' 未找到\"\n",
    "\n",
    "    start_pos = table_start.end()\n",
    "\n",
    "    # 查找表的结束位置（匹配对应的]）\n",
    "    bracket_count = 1\n",
    "    current_pos = start_pos\n",
    "\n",
    "    while bracket_count > 0 and current_pos < len(md_content):\n",
    "        if md_content[current_pos] == '[':\n",
    "            bracket_count += 1\n",
    "        elif md_content[current_pos] == ']':\n",
    "            bracket_count -= 1\n",
    "        current_pos += 1\n",
    "\n",
    "    if bracket_count == 0:\n",
    "        table_schema = md_content[start_pos-1:current_pos]  # -1 是为了包含开始的[\n",
    "        return table_schema.strip()\n",
    "    else:\n",
    "        return f\"表 '{table_name}' 的结构不完整\"\n",
    "\n",
    "def load_and_get_schema(md_file_path, table_name):\n",
    "    \"\"\"\n",
    "    加载MD文件并获取指定表的结构\n",
    "\n",
    "    Args:\n",
    "        md_file_path: MD文件路径\n",
    "        table_name: 表名\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(md_file_path, 'r', encoding='utf-8') as file:\n",
    "            md_content = file.read()\n",
    "\n",
    "        schema = get_table_schema(md_content, table_name)\n",
    "        return schema\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return f\"错误：找不到文件 {md_file_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"读取文件时出错：{e}\"\n",
    "\n",
    "md_file = \"final_algorithm_competition_schema.md\"  # 替换为你的MD文件路径\n",
    "# table_name = \"dwd_argothek_abilityinfo_effects_hi\"  # 替换为你要查询的表名\n",
    "#\n",
    "# schema_str = load_and_get_schema(md_file, table_name)\n",
    "# print(schema_str)"
   ],
   "id": "e6771d0346e22963",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2.加载输入json并解析，构建prompt",
   "id": "387b2b3ae6648f02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "def generate_sql_prompt(json_data, md_file_path, max_schema_length=1000):\n",
    "    \"\"\"\n",
    "    根据JSON数据生成Text-to-SQL的prompt\n",
    "\n",
    "    Args:\n",
    "        json_data: 包含问题、表列表和知识的JSON数据\n",
    "        md_file_path: MD文件路径\n",
    "        max_schema_length: 最大schema长度，超过会截断\n",
    "\n",
    "    Returns:\n",
    "        生成的prompt字符串\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 解析JSON数据\n",
    "        if isinstance(json_data, str):\n",
    "            data = json.loads(json_data)\n",
    "        else:\n",
    "            data = json_data\n",
    "\n",
    "        # 提取关键信息\n",
    "        question = data.get(\"question\", \"\")\n",
    "        table_list = data.get(\"table_list\", [])\n",
    "        knowledge = data.get(\"knowledge\", \"\")\n",
    "\n",
    "        # 读取MD文件内容\n",
    "        with open(md_file_path, 'r', encoding='utf-8') as file:\n",
    "            md_content = file.read()\n",
    "\n",
    "        # 收集所有表的schema\n",
    "        schema_parts = []\n",
    "        for table_name in table_list:\n",
    "            schema = get_table_schema(md_content, table_name)\n",
    "            if schema and not schema.startswith(\"表\"):\n",
    "                # 如果schema太长，进行截断\n",
    "                if len(schema) > max_schema_length:\n",
    "                    schema = schema[:max_schema_length] + \"...\\n(由于长度限制，部分内容已省略)\"\n",
    "                schema_parts.append(f\"表 {table_name} 的结构:\\n{schema}\")\n",
    "\n",
    "        # 构建完整的prompt\n",
    "        prompt_parts = []\n",
    "\n",
    "        # 1. 问题描述\n",
    "        prompt_parts.append(f\"问题: {question}\")\n",
    "\n",
    "        # 2. 表结构\n",
    "        if schema_parts:\n",
    "            prompt_parts.append(\"\\n相关表结构:\")\n",
    "            prompt_parts.extend(schema_parts)\n",
    "\n",
    "        # 3. 业务知识\n",
    "        if knowledge:\n",
    "            prompt_parts.append(f\"\\n业务知识:\\n{knowledge}\")\n",
    "\n",
    "        # 4. 输出要求\n",
    "        prompt_parts.append(\"\\n请根据以上信息生成相应的SQL查询语句。\")\n",
    "\n",
    "        return \"\\n\".join(prompt_parts)\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"生成prompt时出错: {e}\"\n",
    "\n",
    "# 示例JSON数据\n",
    "json_input = {\n",
    "    \"sql_id\": \"sql_1\",\n",
    "    \"question\": \"统计2025.07.24的手游全量用户且标签为其他，在竞品业务下2025.05.30-2025.07.24的在线时长。\\n输出：suserid、sgamecode、ionlinetime\\n\\n\",\n",
    "    \"复杂度\": \"中等\",\n",
    "    \"table_list\": [\n",
    "        \"dws_mgamejp_login_user_activity_di\",\n",
    "        \"dim_vplayerid_vies_df\"\n",
    "    ],\n",
    "    \"knowledge\": \"竞品业务：\\nsgamecode in (\\\"initiatived\\\",\\\"jordass\\\",\\\"esports\\\",\\\"allianceforce\\\",\\\"strategy\\\",\\\"playzone\\\",\\\"su\\\")\\nsaccounttype = \\\"-100\\\" -- 账号体系，取-100表示汇总\\nand suseridtype in (\\\"qq\\\",\\\"wxid\\\") -- 用户类型\\nand splattype = \\\"-100\\\" -- 平台类型\\nand splat = \\\"-100\\\" -- 平台，写死为-100\\n\"\n",
    "}\n",
    "\n",
    "prompt = generate_sql_prompt(json_input, md_file)\n",
    "print(prompt)"
   ],
   "id": "921ca2e735463f69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3.加载json文件",
   "id": "faac2bbe6f9473a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def batch_generate_prompts(json_array_file, md_file_path, output_file=None, max_schema_length=1000):\n",
    "    \"\"\"\n",
    "    批量处理JSON数组文件，为每个对象生成prompt\n",
    "\n",
    "    Args:\n",
    "        json_array_file: JSON数组文件路径\n",
    "        md_file_path: MD文件路径\n",
    "        output_file: 输出文件路径，如果为None则自动生成\n",
    "        max_schema_length: 最大schema长度\n",
    "\n",
    "    Returns:\n",
    "        生成的prompt数量\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 读取JSON数组文件\n",
    "        with open(json_array_file, 'r', encoding='utf-8') as file:\n",
    "            data_array = json.load(file)\n",
    "\n",
    "        # 读取MD文件内容\n",
    "        with open(md_file_path, 'r', encoding='utf-8') as file:\n",
    "            md_content = file.read()\n",
    "\n",
    "        # 生成输出文件名\n",
    "        if output_file is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_file = f\"sql_prompts_{timestamp}.txt\"\n",
    "\n",
    "        # 生成所有prompt\n",
    "        prompts = []\n",
    "        for i, data in enumerate(data_array, 1):\n",
    "            print(f\"正在处理第 {i}/{len(data_array)} 个对象...\")\n",
    "\n",
    "            prompt = generate_single_prompt(data, md_content, max_schema_length)\n",
    "            prompts.append(prompt)\n",
    "\n",
    "        # 保存到文件\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            for i, prompt in enumerate(prompts, 1):\n",
    "                file.write(f\"=== Prompt {i} (SQL ID: {data_array[i-1].get('sql_id', 'N/A')}) ===\\n\")\n",
    "                file.write(prompt)\n",
    "                file.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "        print(f\"成功生成 {len(prompts)} 个prompt，已保存到: {output_file}\")\n",
    "        return len(prompts)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"批量处理时出错: {e}\")\n",
    "        return 0\n",
    "\n",
    "def generate_single_prompt(data, md_content, max_schema_length=1000):\n",
    "    \"\"\"\n",
    "    为单个JSON对象生成prompt\n",
    "\n",
    "    Args:\n",
    "        data: 单个JSON对象\n",
    "        md_content: MD文件内容\n",
    "        max_schema_length: 最大schema长度\n",
    "\n",
    "    Returns:\n",
    "        生成的prompt字符串\n",
    "    \"\"\"\n",
    "    # 提取关键信息\n",
    "    question = data.get(\"question\", \"\")\n",
    "    table_list = data.get(\"table_list\", [])\n",
    "    knowledge = data.get(\"knowledge\", \"\")\n",
    "    complexity = data.get(\"复杂度\", \"\")\n",
    "\n",
    "    # 收集所有表的schema\n",
    "    schema_parts = []\n",
    "    for table_name in table_list:\n",
    "        schema = get_table_schema(md_content, table_name)\n",
    "        if schema and not schema.startswith(\"表\"):\n",
    "            # 如果schema太长，进行截断\n",
    "            if len(schema) > max_schema_length:\n",
    "                schema = schema[:max_schema_length] + \"...\\n(由于长度限制，部分内容已省略)\"\n",
    "            schema_parts.append(f\"表 {table_name} 的结构:\\n{schema}\")\n",
    "\n",
    "    # 构建完整的prompt\n",
    "    prompt_parts = []\n",
    "\n",
    "    # 1. 问题描述\n",
    "    prompt_parts.append(f\"问题: {question}\")\n",
    "\n",
    "    # 2. 复杂度信息\n",
    "    if complexity:\n",
    "        prompt_parts.append(f\"复杂度: {complexity}\")\n",
    "\n",
    "    # 3. 表结构\n",
    "    if schema_parts:\n",
    "        prompt_parts.append(\"\\n相关表结构:\")\n",
    "        prompt_parts.extend(schema_parts)\n",
    "\n",
    "    # 4. 业务知识\n",
    "    if knowledge:\n",
    "        prompt_parts.append(f\"\\n业务知识:\\n{knowledge}\")\n",
    "\n",
    "    # 5. 输出要求\n",
    "    prompt_parts.append(\"\\n请根据以上信息生成相应的SQL查询语句。\")\n",
    "\n",
    "    return \"\\n\".join(prompt_parts)\n",
    "# 文件路径配置\n",
    "json_array_file = \"final_dataset.json\"  # 替换为你的JSON数组文件路径\n",
    "output_file = \"sql_prompts.txt\"  # 可选的输出文件路径，如果为None则自动生成\n",
    "\n",
    "# 批量生成prompt\n",
    "count = batch_generate_prompts(json_array_file, md_file, output_file)\n",
    "\n",
    "if count > 0:\n",
    "    print(f\"处理完成！共生成 {count} 个prompt。\")\n",
    "else:\n",
    "    print(\"处理失败！\")"
   ],
   "id": "d17c135956503e4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4.分类/分解（这个没做）",
   "id": "60ad9f2d2cf015e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "5.自校正（暂时没做）",
   "id": "deb3989e6f00d694"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6.批量生成结果",
   "id": "f0fbd3c76e49a7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def parse_prompts_file(prompts_file):\n",
    "    \"\"\"\n",
    "    解析prompts文件，提取每个prompt块\n",
    "\n",
    "    Args:\n",
    "        prompts_file: prompts文件路径\n",
    "\n",
    "    Returns:\n",
    "        包含所有prompt块的列表，每个块是一个字典\n",
    "    \"\"\"\n",
    "    with open(prompts_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # 使用正则表达式分割不同的prompt块\n",
    "    # 每个prompt块以 \"=== Prompt X (SQL ID: sql_X) ===\" 开始，以 \"=\" * 80 结束\n",
    "    pattern = r'=== Prompt \\d+ \\(SQL ID: (sql_\\d+)\\) ===(.*?)' + re.escape('=' * 80)\n",
    "    matches = re.findall(pattern, content, re.DOTALL)\n",
    "\n",
    "    prompts = []\n",
    "    for sql_id, prompt_content in matches:\n",
    "        prompts.append({\n",
    "            'sql_id': sql_id.strip(),\n",
    "            'content': prompt_content.strip()\n",
    "        })\n",
    "\n",
    "    return prompts\n",
    "\n",
    "def generate_sql_from_prompts(prompts_file, model, tokenizer, output_file=None, device=None):\n",
    "    \"\"\"\n",
    "    使用已加载的模型从prompts文件生成SQL语句\n",
    "\n",
    "    Args:\n",
    "        prompts_file: 包含prompts的文件路径\n",
    "        model: 已加载的模型\n",
    "        tokenizer: 已加载的tokenizer\n",
    "        output_file: 输出文件路径，如果为None则自动生成\n",
    "        device: 使用的设备，如果为None则自动选择\n",
    "\n",
    "    Returns:\n",
    "        生成的SQL数量\n",
    "    \"\"\"\n",
    "    # 设置设备\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"使用设备: {device}\")\n",
    "\n",
    "    # 设置生成参数\n",
    "    generation_config = {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"do_sample\": False,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "\n",
    "    # 解析prompts文件\n",
    "    prompts = parse_prompts_file(prompts_file)\n",
    "    print(f\"找到 {len(prompts)} 个prompts\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    print(\"开始批量生成SQL...\")\n",
    "    for prompt in tqdm(prompts):\n",
    "        sql_id = prompt['sql_id']\n",
    "        prompt_text = prompt['content']\n",
    "\n",
    "        # 生成SQL\n",
    "        try:\n",
    "            # 编码输入\n",
    "            inputs = tokenizer(prompt_text, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "            # 生成SQL\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    **generation_config\n",
    "                )\n",
    "\n",
    "            # 解码输出\n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # 提取SQL部分（去掉prompt部分）\n",
    "            sql = generated_text[len(prompt_text):].strip()\n",
    "\n",
    "            results.append({\n",
    "                'sql_id': sql_id,\n",
    "                'prompt': prompt_text,\n",
    "                'sql': sql\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"生成SQL时出错 (SQL ID: {sql_id}): {e}\")\n",
    "            results.append({\n",
    "                'sql_id': sql_id,\n",
    "                'prompt': prompt_text,\n",
    "                'sql': f\"ERROR: {str(e)}\"\n",
    "            })\n",
    "\n",
    "    # 生成输出文件名\n",
    "    if output_file is None:\n",
    "        import datetime\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f\"generated_sql_{timestamp}.json\"\n",
    "\n",
    "    # 保存结果\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"成功生成 {len(results)} 个SQL语句，已保存到: {output_file}\")\n",
    "    return len(results), results\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 假设 model 和 tokenizer 已经在笔记本中加载\n",
    "\n",
    "    # 配置路径\n",
    "    prompts_file = \"sql_prompts.txt\"  # 替换为你的prompts文件路径\n",
    "    output_file = \"generated_sql_results.json\"  # 可选的输出文件路径\n",
    "\n",
    "    # 生成SQL\n",
    "    count, results = generate_sql_from_prompts(prompts_file, model, tokenizer, output_file)\n",
    "\n",
    "    if count > 0:\n",
    "        print(f\"SQL生成完成！共生成 {count} 个SQL语句。\")\n",
    "\n",
    "        # 打印前几个结果作为示例\n",
    "        print(\"\\n前几个生成结果示例:\")\n",
    "        for i, result in enumerate(results[:3]):\n",
    "            print(f\"\\n--- 结果 {i+1} (SQL ID: {result['sql_id']}) ---\")\n",
    "            print(f\"生成的SQL: {result['gsql']}\")\n",
    "    else:\n",
    "        print(\"SQL生成失败！\")"
   ],
   "id": "c9b5cf5e8ea2897",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "8.整合结果到结果文件里",
   "id": "c0083edb2262f6c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "def merge_sql_to_questions(sql_file, questions_file, output_file):\n",
    "    \"\"\"\n",
    "    将SQL语句根据sql_id合并到问题对象中\n",
    "\n",
    "    Args:\n",
    "        sql_file: 包含SQL语句的JSON文件路径\n",
    "        questions_file: 包含问题的JSON文件路径\n",
    "        output_file: 输出文件路径\n",
    "    \"\"\"\n",
    "\n",
    "    # 读取SQL数据\n",
    "    with open(sql_file, 'r', encoding='utf-8') as f:\n",
    "        sql_data = json.load(f)\n",
    "\n",
    "    # 读取问题数据\n",
    "    with open(questions_file, 'r', encoding='utf-8') as f:\n",
    "        questions_data = json.load(f)\n",
    "\n",
    "    # 创建SQL字典便于查找\n",
    "    sql_dict = {item[\"sql_id\"]: item[\"sql\"] for item in sql_data}\n",
    "\n",
    "    # 合并数据\n",
    "    merged_data = []\n",
    "    for question_item in questions_data:\n",
    "        sql_id = question_item[\"sql_id\"]\n",
    "\n",
    "        # 创建新对象，包含原问题对象的所有属性\n",
    "        merged_item = question_item.copy()\n",
    "\n",
    "        # 添加SQL语句\n",
    "        if sql_id in sql_dict:\n",
    "            merged_item[\"sql\"] = sql_dict[sql_id]\n",
    "        else:\n",
    "            merged_item[\"sql\"] = None  # 或者设置为空字符串 \"\"\n",
    "\n",
    "        merged_data.append(merged_item)\n",
    "\n",
    "    # 保存结果\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(merged_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"合并完成！共处理 {len(merged_data)} 个对象\")\n",
    "    print(f\"结果已保存到: {output_file}\")\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 文件路径 - 请根据实际情况修改\n",
    "    sql_file_path = \"sql_data.json\"  # 包含SQL语句的JSON文件\n",
    "    questions_file_path = \"final_dataset.json\"  # 包含问题的JSON文件\n",
    "    output_file_path = \"merged_data.json\"  # 输出文件\n",
    "\n",
    "    merge_sql_to_questions(sql_file_path, questions_file_path, output_file_path)"
   ],
   "id": "4d23d0132b5cd3b6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
